---
title: "Actividad grupal."
subtitle: "Análisis de un conjunto de datos de origen biológico mediante técnicas de machine
  learning supervisadas y no supervisadas"
author: "Equipo 8, Lote 7: Sandra Benito, Enrique Carnerero, Gabriel Martín, Víctor Saavedra"
date: "29-01-2025"
output: 
  html_document:
    toc: TRUE
    number_sections: TRUE
    toc_float: TRUE
    theme: united
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Reseteo del entorno
rm(list=ls())

# Directorio con los datos
path <- "C:/Users/victo/OneDrive/Escritorio/UNIR/1er - Algoritmos e Inteligencia Artificial/Actividades IA/act3_algIA"
setwd(path)

```


# .	Ambiente y librerías de trabajo


```{r libraries, message=FALSE}
# Carga de librerías generales
library(dplyr)   # Manipulacion de datos
library(stats)   # Para cálculo de matriz de distancias, PCA
library(ggplot2)   # Para representación gráfica

# Carga de librerías específicas
library(Rtsne)   # Para t-SNE
library(RDRToolbox)   # Para Isomap
library(uwot)   # Para UMAP

# Carga de librerías
library(DataExplorer)
library(factoextra)
library(caret)
library(cluster)
library(summarytools)
library(gridExtra)
library(randomForest)
library(knitr)
library(kableExtra)
library(pROC)

```
Las funciones y sus parámetros se desarrollan en la sección de cada método.  


# .	Procesamiento de los datos


```{r data_processing_1, echo=TRUE, message=FALSE}
# Lectura de datos
columns <- read.table('column_names.txt', sep = "\f")   # Nombres de columnas de los datos
data <- read.csv('gene_expression.csv', header=FALSE, col.names=columns[[1]], sep=";")   # Datos
labels <- read.csv('classes.csv', header=FALSE, sep=";")   # Etiquetas para evaluar resultados

# Dataframe para almancenar resultados de clasificacion
data_classes <- data.frame(as.factor(labels$V2))
colnames(data_classes) <- "labels"

```

Los datos provienen de medidas de expresión génica, mediante RNA-seq, de 500 genes realizadas sobre 801 pacientes de cáncer. El archivo consta de 801 filas, correspondientes a los pacientes, y 500 columnas, correspondientes a los genes.  
  
Se adjuntan etiquetas sobre el tipo de cáncer que muestra cada paciente, que se usarán solamente para evaluar los resultados de los algoritmos.  
  

```{r data_processing_2, echo=TRUE, message=FALSE}
# Eliminamos genes con expresión 0 en todos los pacientes
sumas <- colSums(data)
columnascero <- names(sumas[sumas==0])
data <- data[, !names(data) %in% columnascero] # reemplazo el dataset sin esas columnas

# Comprobación de NAs
print(paste0("¿Existe algún valor NA entre los datos? -> ", is.na(sum(colSums(data)))))

```

En el procesamiento inicial de datos hemos eliminado las columnas de genes que no tienen expresión (tienen expresión 0 en todas sus celdas) y hemos comprobado que no existe ningún valor NA.  

  
  
# .	Métodos no supervisados

## Reducción de dimensionalidad

### Red.dim. 1
 
### Red.dim. 2


## Clusterización

Aplicaremos ahora dos técnicas de clusterización.  

### Clusterización no jerárquica por k-means 

En la clusterización no jerárquica suponemos que los datos se estructuran en grupos separados y no anidados unos dentro de otros. En el algoritmo por k-means se busca y optimiza un número k de centroides, que son los puntos medios de cada cluster, de tal manera que los clusters sean lo más compactos posibles (menor distancia de los datos a sus centroides asociados).    

Como método no supervisado, supondremos que no se conoce el grupo al que pertenece cada paciente ni el número de grupos totales. Debemos encontrar entonces el número de centroides óptimo, el número k.  


```{r cluster_kmeans_1, echo = FALSE}

set.seed(1995)

# Carga de librerias
library(factoextra)   # Contiene funciones fviz_clust, fviz_nbclust
library(stats)   # Contiene función kmeans
# library(ggplot2)


## BUSCAMOS EL NUMERO OPTIMO DE CLUSTERS

# Parámetros de fviz_nbclust():
#   x: el dataframe o la matriz de los datos
#   FUNcluster: función de clusterización cuyo 2º parámetro vamos a optimizar (en este caso: 2º parametro de kmeans es centers) 
#   method: método para la optimización ("wss" - whithin sum of square: total de sumas de cuadrados)
# Salida de fviz_nbclust():
#   Un ggplot2

# Mostramos el error del modelo en funcion del numero de clusters
fviz_nbclust(data[, 1:497], kmeans, method = "wss") +
  ggtitle("Número óptimo de clusters", subtitle = "") +
  theme_classic()


```


El gráfico nos indica lo compactos que son los clusters en función del número de centroides (k): a menor total de suma de cuadrados (WSS), más compactos. Para elegir el k óptimo usamos la regla del codo, esto es, elegimos un número de centroides tal que añadir un número más no mejora mucho más el WSS total.  

En este caso el número óptimo sería de 5 centroides, dado que el WSS baja consistentemente hasta k=5 pero no mucho más para k mayores. Realizaremos ahora el clustering por k-means con 5 centroides. Incluimos también una proyección en 2-D del resultado.  


```{r cluster_kmeans_2, echo = FALSE}

## REALIZAMOS CLUSTERING POR K-MEANS

# Parámetros de kmeans():
#   x: el dataframe o la matriz de los datos
#   centers: si es un número, es el número de clusters (se elegirán filas de x al azar como centrómeros iniciales)
#   iter.max: máximo permitido de iteraciones
#   nstart: nº de sets aleatorios de x a elegir al inicio
# Salida de kmeans():
#   Un objeto de clase "kmeans", es una lista que incluye (entre otros):
#     $cluster: vector indicando a qué cluster pertenece cada observación (fila de x)
#     $centers: matriz con las coordenadas multidimensionales de cada centrómero (nºcentrómeros x nºcolumnasdex)

# Agrupamos datos en 5 clusters
kmeans.result <- kmeans(data[, 1:497], centers = 5, iter.max = 100, nstart = 25)



## MOSTRAMOS EL CLUSTERING EN 2-D

# Clustering no jerárquico con kmeans, con función fviz_cluster()
# Parámetros de fviz_cluster():
#   object: objeto de clase "kmeans" (de librería stats) u otra clase soportada.
#   data: el dataframe usado en el clustering (obligatorio si object es kmeans)
#   xlab, ylab: etiquetas de los ejes x,y
# Salida de fviz_cluster():
#   Un ggplot2

# Plot del resultado de 5 clusters en 2D
fviz_cluster(kmeans.result, data[, 1:497], xlab = '', ylab = '') +
  ggtitle("Gráfico k-means, centros = 5", subtitle = "") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

```


Una vez clasificadas las muestras en clusters, y dado que en este caso conocemos el grupo al que pertenece cada paciente, podemos comprobar si la clusterización ha sido adecuada. La siguiente tabla muestra la distribución de las muestras, en las filas el tipo de tumor y en las columnas el número de cluster calculado. Observamos que los grupos reales y los calculados coinciden para casi todas las muestras, y por tanto la predicción de clusters ha sido muy buena.   


```{r cluster_kmeans_3, echo = FALSE}

## CLASIFICACION DE LAS OBSERVACIONES Y EVALUACION DEL MODELO

# Guardamos el numero de cluster calculado para cada observacion en la columna cluster_kmeans
data_classes$cluster_kmeans <- as.factor(kmeans.result$cluster)

# Observamos la relación entre los clusteres calculados y el tipo de tumor
table(data_classes$labels, data_classes$cluster_kmeans)

```

Ventajas de la clusterización por k-means: .   

Limitaciones:    



### Clusterización jerárquica aglomerativa por varianza mínima de Ward

En la clusterización jerárquica suponemos que los datos se estructuran en grupos, y que los grupos se encuentran anidados unos dentro de otros en función de su similitud. Cuando es aglomerativa, los datos se van agrupando "de abajo a arriba", es decir, las muestras se agrupan inicialmente con muestras similares y unos grupos se van fusionando gradualmente con otros grupos en una jerarquía más alta. Existen varios criterios para esta agrupación, entre ellos la varianza mínima de Ward, que busca minimizar la varianza total dentro de los clusters.  


```{r cluster_ward_1, echo = FALSE}

# Carga de librerias
library(stats)   # Contiene funciones dist, hclust
library(factoextra) # Contiene función fviz_dend

# Calculamos la matriz de distancia (input para la funcion hclust)
dist_matrix <- dist(data[, 1:497])

# Clustering jerárquico aglomerativo, con función hclust()
# Parámetros de hclust():
#   d: matriz de distancias
#   method: método de clustering 
# Salida de hclust():
#   Un objeto de clase "hclus", es una lista
hclust_model_ward <- hclust(dist_matrix, method = "ward.D") 

```


La clusterización jerárquica nos permite ordenar y visualizar los datos mediante un dendrograma, o árbol invertido. Cada hoja corresponde a una muestra y las ramas representan los clusters que se van generando al combinar elementos. La altura de la rama indica el grado de diferencia entre los elementos fusionados.  

El dendrograma generado a partir de nuestro dataset es el siguiente:  


```{r cluster_ward_2, echo = FALSE, warning=FALSE}

## DENDROGRAMA

# Creamos dendrograma con la clusterizacion
clust_ward <- fviz_dend(hclust_model_ward, 
                          cex = 0.5,
                          main = "Ward",
                          xlab = "Índice de Observaciones",
                          ylab = "Distancia") + 
                          theme_classic()
# Mostramos el dendrograma
print(clust_ward)


```

Se observan varios clusters claramente separados, donde una mayor separación entre ellos se representa con una mayor altura de rama. La elección del número óptimo de clusters es arbitraria, dependiendo del umbral de distancia que seleccionemos en el eje vertical.  


Podemos comprobar si la clusterización es capaz de predecir correctamente los grupos de pacientes, sabiendo que nuestro dataset está dividido en 5 grupos. En primer lugar, representaremos cada cluster en distinto color en el dendrograma. En segundo lugar, compararemos si los clusters calculados para las muestras coinciden con los tipos de tumor reales.  


```{r cluster_ward_3, echo = FALSE, warning=FALSE}

## DENDROGRAMA DE COLOR

# Paleta de colores
colors <- rainbow(5)

# Creamos dendrograma con la clusterizacion
clust_ward_color <- fviz_dend(hclust_model_ward, 
                          cex = 0.5,
                          k = 5,
                          palette = rainbow(5),
                          main = "Ward",
                          xlab = "Índice de Observaciones",
                          ylab = "Distancia") + 
                          theme_classic()
# Mostramos el dendrograma
print(clust_ward_color)


```


```{r cluster_ward_4, echo = FALSE}

# CLASIFICACION DE LAS OBSERVACIONES Y EVALUACION DEL MODELO

# Con los resultados, clasificamos cada observación (fila) de nuestro dataset en uno de 5 clusters. El numero de cluster se guarda en la columna cluster_ward
data_classes$cluster_ward <- as.factor(cutree(hclust_model_ward, k = 5))

# Observamos cuántos pacientes de cada cluster se han asociado a cada tipo de tumor
table(data_classes$labels, data_classes$cluster_ward)

```

Los grupos reales y los calculados coinciden casi por completo.   


## Preguntas sobre métodos no supervisados:

**¿Cuál es el motivo por el cual habéis seleccionado estas técnicas de reducción de dimensionalidad?** 

**¿Cuál es el motivo por el cual habéis seleccionado estas técnicas de clusterización?**  
Hemos escogido un algoritmo de clusterización jerárquico y otro no jerárquico. La clusterización no jerárquica por k-means, además de resultar en una menor variabilidad dentro de los grupos, nos permite optimizar el número de estos. El método de Ward de clusterización jerárquica resulta también en menor variabilidad dentro de los grupos, sin necesidad de optimizar el número de clústeres, y resulta en una visualiación de tipo dendrograma.   


**En ambos casos, ¿qué aspectos positivos y negativos tienen cada una?**  

En la siguiente tabla se recogen las ventajas y limitaciones de los métodos no supervisados utilizados en la actividad:  

```{r tabla_metodos_no_sup, echo = FALSE}

library(gt)
library(dplyr)

# Crear la tabla con saltos de línea en ventajas y limitaciones
tabla <- data.frame(
  Método = c("Método A", "Método B", "Clusterización por k-means", "Clusterización jerárquica de Ward"),
  Ventajas = c("Ventaja 1<br>Ventaja 2", 
               "Ventaja 1<br>Ventaja 2", 
               "Implementación sencilla. <br>Computacionalmente poco costosa. <br>Resulta en clústeres con baja variabilidad interna. ", 
               "No requiere especificar número de clústeres (frente a k-means). <br>Resistente a valores atípicos (frente otros métodos jerárquicos). "),
  Limitaciones = c("Limitación 1<br>Limitación 2", 
                   "Limitación 1<br>Limitación 2", 
                   "Puede ser difícil optimizar los centroides. <br>Sensible a valores atípicos. <br>Resulta en clústeres con baja variabilidad interna. ", 
                   "Puede ser difícil determinar criterio (dentro de jerárquicos). <br>Cambiar el número de muestras puede alterar la clusterización. ")
)

# Generar la tabla en gt
tabla %>%
  gt() %>%
  tab_header(title = "Comparación de métodos de aprendizaje no supervisado") %>%
  cols_label(Método = "Método", Ventajas = "Ventajas", Limitaciones = "Limitaciones") %>%
  fmt_markdown(columns = c(Ventajas, Limitaciones))  # Permite interpretar los saltos de línea


```


**En el caso de la clusterización, ¿se puede afirmar con certeza que los clústeres generados son los mejores posibles?**  
En el algoritmo por k-means podemos optimizar el número de centroides (k) para asegurarnos de que los clústeres son lo más compactos posibles, y de esta manera tenemos la certeza de que el número de clústeres es óptimo (si el dataset permite que exista un óptimo). En el algoritmo de Ward no se incluye este paso, la elección del número de clústeres es arbitrario y puede no ser el óptimo.  


# .	Métodos supervisados



```{r chunk_name, echo = FALSE}

```
